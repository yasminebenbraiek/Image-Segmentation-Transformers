{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasminebenbraiek/Medical-Transformers/blob/main/Levit_UNet_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lOYA-y3iYmCV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a321a85-57bd-4d79-fa4e-d020b479cd59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jsI-QiX-B-us"
      },
      "outputs": [],
      "source": [
        "!pip install timm\n",
        "!pip install apex\n",
        "!pip install ptflops\n",
        "!pip install utils"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Load data\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras.layers import Input, UpSampling2D, Activation, Conv2D, Concatenate, BatchNormalization, LayerNormalization, Dropout, Dense\n",
        "from keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "# Function to read grayscale images and masks from a directory\n",
        "def read_images_and_masks(directory, target_size):\n",
        "    images = []\n",
        "    masks = []\n",
        "\n",
        "    # Iterate through the folders in the directory\n",
        "    for folder_name in os.listdir(directory):\n",
        "        folder_path = os.path.join(directory, folder_name)\n",
        "\n",
        "        # Check if it's a directory\n",
        "        if os.path.isdir(folder_path):\n",
        "            image_folder = os.path.join(folder_path, 'images')\n",
        "            mask_folder = os.path.join(folder_path, 'masks')\n",
        "\n",
        "            # Iterate through the files in the image folder\n",
        "            for filename in os.listdir(image_folder):\n",
        "                if filename.endswith(\".png\"):\n",
        "                    # Read the image as grayscale\n",
        "                    image_path = os.path.join(image_folder, filename)\n",
        "                    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
        "                    image = image.resize(target_size)\n",
        "                    images.append(np.array(image))\n",
        "\n",
        "                    # Read the corresponding mask as grayscale\n",
        "                    mask_filename = filename\n",
        "                    mask_path = os.path.join(mask_folder, mask_filename)\n",
        "                    mask = Image.open(mask_path).convert('L')  # Convert to grayscale\n",
        "                    mask = mask.resize(target_size)\n",
        "                    masks.append(np.array(mask))\n",
        "\n",
        "    # Reshape the images and masks\n",
        "    images = np.array(images)\n",
        "    masks = np.array(masks)\n",
        "    images = np.expand_dims(images, axis=-1)  # Expand dimensions to (224, 224, 1)\n",
        "\n",
        "    return images, masks\n",
        "\n",
        "train_image_directory = \"#\"\n",
        "\n",
        "# Read training images and masks\n",
        "target_size = (224, 224)  # Adjust the target size as per your requirement\n",
        "train_images, train_masks = read_images_and_masks(train_image_directory, target_size)\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "#train_images, val_images, train_masks, val_masks = train_test_split(\n",
        " #   train_images, train_masks, test_size=0.2, random_state=42\n",
        "#)\n"
      ],
      "metadata": {
        "id": "FG0XtSAeo54S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Utils\n",
        "# Copyright (c) 2015-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "\"\"\"\n",
        "Misc functions, including distributed helpers.\n",
        "Mostly copy-paste from torchvision references.\n",
        "\"\"\"\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "import datetime\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "\n",
        "class SmoothedValue(object):\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total],\n",
        "                         dtype=torch.float64, device='cuda')\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median,\n",
        "            avg=self.avg,\n",
        "            global_avg=self.global_avg,\n",
        "            max=self.max,\n",
        "            value=self.value)\n",
        "\n",
        "\n",
        "class MetricLogger(object):\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
        "            type(self).__name__, attr))\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(\n",
        "                \"{}: {}\".format(name, str(meter))\n",
        "            )\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = ''\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
        "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
        "        log_msg = [\n",
        "            header,\n",
        "            '[{0' + space_fmt + '}/{1}]',\n",
        "            'eta: {eta}',\n",
        "            '{meters}',\n",
        "            'time: {time}',\n",
        "            'data: {data}'\n",
        "        ]\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg.append('max mem: {memory:.0f}')\n",
        "        log_msg = self.delimiter.join(log_msg)\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time),\n",
        "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
        "                else:\n",
        "                    print(log_msg.format(\n",
        "                        i, len(iterable), eta=eta_string,\n",
        "                        meters=str(self),\n",
        "                        time=str(iter_time), data=str(data_time)))\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
        "            header, total_time_str, total_time / len(iterable)))\n",
        "\n",
        "\n",
        "def _load_checkpoint_for_ema(model_ema, checkpoint):\n",
        "    \"\"\"\n",
        "    Workaround for ModelEma._load_checkpoint to accept an already-loaded object\n",
        "    \"\"\"\n",
        "    mem_file = io.BytesIO()\n",
        "    torch.save(checkpoint, mem_file)\n",
        "    mem_file.seek(0)\n",
        "    model_ema._load_checkpoint(mem_file)\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop('force', False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
        "        args.rank = int(os.environ[\"RANK\"])\n",
        "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
        "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
        "    elif 'SLURM_PROCID' in os.environ:\n",
        "        args.rank = int(os.environ['SLURM_PROCID'])\n",
        "        args.gpu = args.rank % torch.cuda.device_count()\n",
        "    else:\n",
        "        print('Not using distributed mode')\n",
        "        args.distributed = False\n",
        "        return\n",
        "\n",
        "    args.distributed = True\n",
        "\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    args.dist_backend = 'nccl'\n",
        "    print('| distributed init (rank {}): {}'.format(\n",
        "        args.rank, args.dist_url), flush=True)\n",
        "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
        "                                         world_size=args.world_size, rank=args.rank)\n",
        "    torch.distributed.barrier()\n",
        "    setup_for_distributed(args.rank == 0)\n",
        "\n",
        "\n",
        "def replace_batchnorm(net):\n",
        "    for child_name, child in net.named_children():\n",
        "        if hasattr(child, 'fuse'):\n",
        "            setattr(net, child_name, child.fuse())\n",
        "        elif isinstance(child, torch.nn.Conv2d):\n",
        "            child.bias = torch.nn.Parameter(torch.zeros(child.weight.size(0)))\n",
        "        elif isinstance(child, torch.nn.BatchNorm2d):\n",
        "            setattr(net, child_name, torch.nn.Identity())\n",
        "        else:\n",
        "            replace_batchnorm(child)\n",
        "\n",
        "\n",
        "def replace_layernorm(net):\n",
        "    import apex\n",
        "    for child_name, child in net.named_children():\n",
        "        if isinstance(child, torch.nn.LayerNorm):\n",
        "            setattr(net, child_name, apex.normalization.FusedLayerNorm(\n",
        "                child.weight.size(0)))\n",
        "        else:\n",
        "            replace_layernorm(child)\n"
      ],
      "metadata": {
        "id": "UWxtLZJcpHVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title LeVit\n",
        "# Copyright (c) 2015-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "\n",
        "# Modified from\n",
        "# https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
        "# Copyright 2020 Ross Wightman, Apache-2.0 License\n",
        "\n",
        "import torch\n",
        "import itertools\n",
        "import utils\n",
        "\n",
        "from timm.models.vision_transformer import trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "specification = {\n",
        "    'LeViT_128S': {\n",
        "        'C': '128_256_384', 'D': 16, 'N': '4_6_8', 'X': '2_3_4', 'drop_path': 0,\n",
        "        'weights': 'https://dl.fbaipublicfiles.com/LeViT/LeViT-128S-96703c44.pth'},\n",
        "    'LeViT_128': {\n",
        "        'C': '128_256_384', 'D': 16, 'N': '4_8_12', 'X': '4_4_4', 'drop_path': 0,\n",
        "        'weights': 'https://dl.fbaipublicfiles.com/LeViT/LeViT-128-b88c2750.pth'},\n",
        "    'LeViT_192': {\n",
        "        'C': '192_288_384', 'D': 32, 'N': '3_5_6', 'X': '4_4_4', 'drop_path': 0,\n",
        "        'weights': 'https://dl.fbaipublicfiles.com/LeViT/LeViT-192-92712e41.pth'},\n",
        "    'LeViT_256': {\n",
        "        'C': '256_384_512', 'D': 32, 'N': '4_6_8', 'X': '4_4_4', 'drop_path': 0,\n",
        "        'weights': 'https://dl.fbaipublicfiles.com/LeViT/LeViT-256-13b5763e.pth'},\n",
        "    'LeViT_384': {\n",
        "        'C': '384_512_768', 'D': 32, 'N': '6_9_12', 'X': '4_4_4', 'drop_path': 0.1,\n",
        "        'weights': 'https://dl.fbaipublicfiles.com/LeViT/LeViT-384-9bdaf2e2.pth'},\n",
        "}\n",
        "\n",
        "__all__ = [specification.keys()]\n",
        "\n",
        "\n",
        "@register_model\n",
        "def LeViT_128S(num_classes=1000, distillation=True,\n",
        "               pretrained=False, pretrained_cfg=None, fuse=False):\n",
        "    return model_factory(**specification['LeViT_128S'], num_classes=num_classes,\n",
        "                         distillation=distillation, pretrained=pretrained, fuse=fuse)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def LeViT_128(num_classes=1000, distillation=True,\n",
        "              pretrained=False, pretrained_cfg=None, fuse=False):\n",
        "    return model_factory(**specification['LeViT_128'], num_classes=num_classes,\n",
        "                         distillation=distillation, pretrained=pretrained, fuse=fuse)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def LeViT_192(num_classes=1000, distillation=True,\n",
        "              pretrained=False, pretrained_cfg=None, fuse=False):\n",
        "    return model_factory(**specification['LeViT_192'], num_classes=num_classes,\n",
        "                         distillation=distillation, pretrained=pretrained, fuse=fuse)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def LeViT_256(num_classes=1000, distillation=True,\n",
        "              pretrained=False, pretrained_cfg=None, fuse=False):\n",
        "    return model_factory(**specification['LeViT_256'], num_classes=num_classes,\n",
        "                         distillation=distillation, pretrained=pretrained, fuse=fuse)\n",
        "\n",
        "\n",
        "@register_model\n",
        "def LeViT_384(num_classes=1000, distillation=True,\n",
        "              pretrained=False, pretrained_cfg=None, fuse=False):\n",
        "    return model_factory(**specification['LeViT_384'], num_classes=num_classes,\n",
        "                         distillation=distillation, pretrained=pretrained, fuse=fuse)\n",
        "\n",
        "\n",
        "FLOPS_COUNTER = 0\n",
        "\n",
        "\n",
        "class Conv2d_BN(torch.nn.Sequential):\n",
        "    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n",
        "                 groups=1, bn_weight_init=1, resolution=-10000):\n",
        "        super().__init__()\n",
        "        self.add_module('c', torch.nn.Conv2d(\n",
        "            a, b, ks, stride, pad, dilation, groups, bias=False))\n",
        "        bn = torch.nn.BatchNorm2d(b)\n",
        "        torch.nn.init.constant_(bn.weight, bn_weight_init)\n",
        "        torch.nn.init.constant_(bn.bias, 0)\n",
        "        self.add_module('bn', bn)\n",
        "\n",
        "        global FLOPS_COUNTER\n",
        "        output_points = ((resolution + 2 * pad - dilation *\n",
        "                          (ks - 1) - 1) // stride + 1)**2\n",
        "        FLOPS_COUNTER += a * b * output_points * (ks**2) // groups\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fuse(self):\n",
        "        c, bn = self._modules.values()\n",
        "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
        "        w = c.weight * w[:, None, None, None]\n",
        "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
        "            (bn.running_var + bn.eps)**0.5\n",
        "        m = torch.nn.Conv2d(w.size(1) * self.c.groups, w.size(\n",
        "            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n",
        "        m.weight.data.copy_(w)\n",
        "        m.bias.data.copy_(b)\n",
        "        return m\n",
        "\n",
        "\n",
        "class Linear_BN(torch.nn.Sequential):\n",
        "    def __init__(self, a, b, bn_weight_init=1, resolution=-100000):\n",
        "        super().__init__()\n",
        "        self.add_module('c', torch.nn.Linear(a, b, bias=False))\n",
        "        bn = torch.nn.BatchNorm1d(b)\n",
        "        torch.nn.init.constant_(bn.weight, bn_weight_init)\n",
        "        torch.nn.init.constant_(bn.bias, 0)\n",
        "        self.add_module('bn', bn)\n",
        "\n",
        "        global FLOPS_COUNTER\n",
        "        output_points = resolution**2\n",
        "        FLOPS_COUNTER += a * b * output_points\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fuse(self):\n",
        "        l, bn = self._modules.values()\n",
        "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
        "        w = l.weight * w[:, None]\n",
        "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
        "            (bn.running_var + bn.eps)**0.5\n",
        "        m = torch.nn.Linear(w.size(1), w.size(0))\n",
        "        m.weight.data.copy_(w)\n",
        "        m.bias.data.copy_(b)\n",
        "        return m\n",
        "\n",
        "    def forward(self, x):\n",
        "        l, bn = self._modules.values()\n",
        "        x = l(x)\n",
        "        return bn(x.flatten(0, 1)).reshape_as(x)\n",
        "\n",
        "\n",
        "class BN_Linear(torch.nn.Sequential):\n",
        "    def __init__(self, a, b, bias=True, std=0.02):\n",
        "        super().__init__()\n",
        "        self.add_module('bn', torch.nn.BatchNorm1d(a))\n",
        "        l = torch.nn.Linear(a, b, bias=bias)\n",
        "        trunc_normal_(l.weight, std=std)\n",
        "        if bias:\n",
        "            torch.nn.init.constant_(l.bias, 0)\n",
        "        self.add_module('l', l)\n",
        "        global FLOPS_COUNTER\n",
        "        FLOPS_COUNTER += a * b\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fuse(self):\n",
        "        bn, l = self._modules.values()\n",
        "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
        "        b = bn.bias - self.bn.running_mean * \\\n",
        "            self.bn.weight / (bn.running_var + bn.eps)**0.5\n",
        "        w = l.weight * w[None, :]\n",
        "        if l.bias is None:\n",
        "            b = b @ self.l.weight.T\n",
        "        else:\n",
        "            b = (l.weight @ b[:, None]).view(-1) + self.l.bias\n",
        "        m = torch.nn.Linear(w.size(1), w.size(0))\n",
        "        m.weight.data.copy_(w)\n",
        "        m.bias.data.copy_(b)\n",
        "        return m\n",
        "\n",
        "\n",
        "def b16(n, activation, resolution=224):\n",
        "    return torch.nn.Sequential(\n",
        "        Conv2d_BN(3, n // 8, 3, 2, 1, resolution=resolution),\n",
        "        activation(),\n",
        "        Conv2d_BN(n // 8, n // 4, 3, 2, 1, resolution=resolution // 2),\n",
        "        activation(),\n",
        "        Conv2d_BN(n // 4, n // 2, 3, 2, 1, resolution=resolution // 4),\n",
        "        activation(),\n",
        "        Conv2d_BN(n // 2, n, 3, 2, 1, resolution=resolution // 8))\n",
        "\n",
        "\n",
        "class Residual(torch.nn.Module):\n",
        "    def __init__(self, m, drop):\n",
        "        super().__init__()\n",
        "        self.m = m\n",
        "        self.drop = drop\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training and self.drop > 0:\n",
        "            return x + self.m(x) * torch.rand(x.size(0), 1, 1,\n",
        "                                              device=x.device).ge_(self.drop).div(1 - self.drop).detach()\n",
        "        else:\n",
        "            return x + self.m(x)\n",
        "\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self, dim, key_dim, num_heads=8,\n",
        "                 attn_ratio=4,\n",
        "                 activation=None,\n",
        "                 resolution=14):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = key_dim ** -0.5\n",
        "        self.key_dim = key_dim\n",
        "        self.nh_kd = nh_kd = key_dim * num_heads\n",
        "        self.d = int(attn_ratio * key_dim)\n",
        "        self.dh = int(attn_ratio * key_dim) * num_heads\n",
        "        self.attn_ratio = attn_ratio\n",
        "        h = self.dh + nh_kd * 2\n",
        "        self.qkv = Linear_BN(dim, h, resolution=resolution)\n",
        "        self.proj = torch.nn.Sequential(activation(), Linear_BN(\n",
        "            self.dh, dim, bn_weight_init=0, resolution=resolution))\n",
        "\n",
        "        points = list(itertools.product(range(resolution), range(resolution)))\n",
        "        N = len(points)\n",
        "        attention_offsets = {}\n",
        "        idxs = []\n",
        "        for p1 in points:\n",
        "            for p2 in points:\n",
        "                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
        "                if offset not in attention_offsets:\n",
        "                    attention_offsets[offset] = len(attention_offsets)\n",
        "                idxs.append(attention_offsets[offset])\n",
        "        self.attention_biases = torch.nn.Parameter(\n",
        "            torch.zeros(num_heads, len(attention_offsets)))\n",
        "        self.register_buffer('attention_bias_idxs',\n",
        "                             torch.LongTensor(idxs).view(N, N))\n",
        "\n",
        "        global FLOPS_COUNTER\n",
        "        #queries * keys\n",
        "        FLOPS_COUNTER += num_heads * (resolution**4) * key_dim\n",
        "        # softmax\n",
        "        FLOPS_COUNTER += num_heads * (resolution**4)\n",
        "        #attention * v\n",
        "        FLOPS_COUNTER += num_heads * self.d * (resolution**4)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def train(self, mode=True):\n",
        "        super().train(mode)\n",
        "        if mode and hasattr(self, 'ab'):\n",
        "            del self.ab\n",
        "        else:\n",
        "            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n",
        "\n",
        "    def forward(self, x):  # x (B,N,C)\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.view(B, N, self.num_heads, -\n",
        "                           1).split([self.key_dim, self.key_dim, self.d], dim=3)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        attn = (\n",
        "            (q @ k.transpose(-2, -1)) * self.scale\n",
        "            +\n",
        "            (self.attention_biases[:, self.attention_bias_idxs]\n",
        "             if self.training else self.ab)\n",
        "        )\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, self.dh)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Subsample(torch.nn.Module):\n",
        "    def __init__(self, stride, resolution):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        self.resolution = resolution\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        x = x.view(B, self.resolution, self.resolution, C)[\n",
        "            :, ::self.stride, ::self.stride].reshape(B, -1, C)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionSubsample(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, key_dim, num_heads=8,\n",
        "                 attn_ratio=2,\n",
        "                 activation=None,\n",
        "                 stride=2,\n",
        "                 resolution=14, resolution_=7):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = key_dim ** -0.5\n",
        "        self.key_dim = key_dim\n",
        "        self.nh_kd = nh_kd = key_dim * num_heads\n",
        "        self.d = int(attn_ratio * key_dim)\n",
        "        self.dh = int(attn_ratio * key_dim) * self.num_heads\n",
        "        self.attn_ratio = attn_ratio\n",
        "        self.resolution_ = resolution_\n",
        "        self.resolution_2 = resolution_**2\n",
        "        h = self.dh + nh_kd\n",
        "        self.kv = Linear_BN(in_dim, h, resolution=resolution)\n",
        "\n",
        "        self.q = torch.nn.Sequential(\n",
        "            Subsample(stride, resolution),\n",
        "            Linear_BN(in_dim, nh_kd, resolution=resolution_))\n",
        "        self.proj = torch.nn.Sequential(activation(), Linear_BN(\n",
        "            self.dh, out_dim, resolution=resolution_))\n",
        "\n",
        "        self.stride = stride\n",
        "        self.resolution = resolution\n",
        "        points = list(itertools.product(range(resolution), range(resolution)))\n",
        "        points_ = list(itertools.product(\n",
        "            range(resolution_), range(resolution_)))\n",
        "        N = len(points)\n",
        "        N_ = len(points_)\n",
        "        attention_offsets = {}\n",
        "        idxs = []\n",
        "        for p1 in points_:\n",
        "            for p2 in points:\n",
        "                size = 1\n",
        "                offset = (\n",
        "                    abs(p1[0] * stride - p2[0] + (size - 1) / 2),\n",
        "                    abs(p1[1] * stride - p2[1] + (size - 1) / 2))\n",
        "                if offset not in attention_offsets:\n",
        "                    attention_offsets[offset] = len(attention_offsets)\n",
        "                idxs.append(attention_offsets[offset])\n",
        "        self.attention_biases = torch.nn.Parameter(\n",
        "            torch.zeros(num_heads, len(attention_offsets)))\n",
        "        self.register_buffer('attention_bias_idxs',\n",
        "                             torch.LongTensor(idxs).view(N_, N))\n",
        "\n",
        "        global FLOPS_COUNTER\n",
        "        #queries * keys\n",
        "        FLOPS_COUNTER += num_heads * \\\n",
        "            (resolution**2) * (resolution_**2) * key_dim\n",
        "        # softmax\n",
        "        FLOPS_COUNTER += num_heads * (resolution**2) * (resolution_**2)\n",
        "        #attention * v\n",
        "        FLOPS_COUNTER += num_heads * \\\n",
        "            (resolution**2) * (resolution_**2) * self.d\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def train(self, mode=True):\n",
        "        super().train(mode)\n",
        "        if mode and hasattr(self, 'ab'):\n",
        "            del self.ab\n",
        "        else:\n",
        "            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        k, v = self.kv(x).view(B, N, self.num_heads, -\n",
        "                               1).split([self.key_dim, self.d], dim=3)\n",
        "        k = k.permute(0, 2, 1, 3)  # BHNC\n",
        "        v = v.permute(0, 2, 1, 3)  # BHNC\n",
        "        q = self.q(x).view(B, self.resolution_2, self.num_heads,\n",
        "                           self.key_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale + \\\n",
        "            (self.attention_biases[:, self.attention_bias_idxs]\n",
        "             if self.training else self.ab)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, -1, self.dh)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class LeViT(torch.nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224,\n",
        "                 patch_size=16,\n",
        "                 in_chans=3,\n",
        "                 num_classes=1000,\n",
        "                 embed_dim=[192],\n",
        "                 key_dim=[64],\n",
        "                 depth=[12],\n",
        "                 num_heads=[3],\n",
        "                 attn_ratio=[2],\n",
        "                 mlp_ratio=[2],\n",
        "                 hybrid_backbone=None,\n",
        "                 down_ops=[],\n",
        "                 attention_activation=torch.nn.Hardswish,\n",
        "                 mlp_activation=torch.nn.Hardswish,\n",
        "                 distillation=True,\n",
        "                 drop_path=0):\n",
        "        super().__init__()\n",
        "        global FLOPS_COUNTER\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = embed_dim[-1]\n",
        "        self.embed_dim = embed_dim\n",
        "        self.distillation = distillation\n",
        "\n",
        "        self.patch_embed = hybrid_backbone\n",
        "\n",
        "        self.blocks = []\n",
        "        down_ops.append([''])\n",
        "        resolution = img_size // patch_size\n",
        "        for i, (ed, kd, dpth, nh, ar, mr, do) in enumerate(\n",
        "                zip(embed_dim, key_dim, depth, num_heads, attn_ratio, mlp_ratio, down_ops)):\n",
        "            for _ in range(dpth):\n",
        "                self.blocks.append(\n",
        "                    Residual(Attention(\n",
        "                        ed, kd, nh,\n",
        "                        attn_ratio=ar,\n",
        "                        activation=attention_activation,\n",
        "                        resolution=resolution,\n",
        "                    ), drop_path))\n",
        "                if mr > 0:\n",
        "                    h = int(ed * mr)\n",
        "                    self.blocks.append(\n",
        "                        Residual(torch.nn.Sequential(\n",
        "                            Linear_BN(ed, h, resolution=resolution),\n",
        "                            mlp_activation(),\n",
        "                            Linear_BN(h, ed, bn_weight_init=0,\n",
        "                                      resolution=resolution),\n",
        "                        ), drop_path))\n",
        "            if do[0] == 'Subsample':\n",
        "                #('Subsample',key_dim, num_heads, attn_ratio, mlp_ratio, stride)\n",
        "                resolution_ = (resolution - 1) // do[5] + 1\n",
        "                self.blocks.append(\n",
        "                    AttentionSubsample(\n",
        "                        *embed_dim[i:i + 2], key_dim=do[1], num_heads=do[2],\n",
        "                        attn_ratio=do[3],\n",
        "                        activation=attention_activation,\n",
        "                        stride=do[5],\n",
        "                        resolution=resolution,\n",
        "                        resolution_=resolution_))\n",
        "                resolution = resolution_\n",
        "                if do[4] > 0:  # mlp_ratio\n",
        "                    h = int(embed_dim[i + 1] * do[4])\n",
        "                    self.blocks.append(\n",
        "                        Residual(torch.nn.Sequential(\n",
        "                            Linear_BN(embed_dim[i + 1], h,\n",
        "                                      resolution=resolution),\n",
        "                            mlp_activation(),\n",
        "                            Linear_BN(\n",
        "                                h, embed_dim[i + 1], bn_weight_init=0, resolution=resolution),\n",
        "                        ), drop_path))\n",
        "        self.blocks = torch.nn.Sequential(*self.blocks)\n",
        "\n",
        "        # Classifier head\n",
        "        self.head = BN_Linear(\n",
        "            embed_dim[-1], num_classes) if num_classes > 0 else torch.nn.Identity()\n",
        "        if distillation:\n",
        "            self.head_dist = BN_Linear(\n",
        "                embed_dim[-1], num_classes) if num_classes > 0 else torch.nn.Identity()\n",
        "\n",
        "        self.FLOPS = FLOPS_COUNTER\n",
        "        FLOPS_COUNTER = 0\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {x for x in self.state_dict().keys() if 'attention_biases' in x}\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        x = self.blocks(x)\n",
        "        x = x.mean(1)\n",
        "        if self.distillation:\n",
        "            x = self.head(x), self.head_dist(x)\n",
        "            if not self.training:\n",
        "                x = (x[0] + x[1]) / 2\n",
        "        else:\n",
        "            x = self.head(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "def model_factory(C, D, X, N, drop_path, weights,\n",
        "                  num_classes, distillation, pretrained, fuse):\n",
        "    embed_dim = [int(x) for x in C.split('_')]\n",
        "    num_heads = [int(x) for x in N.split('_')]\n",
        "    depth = [int(x) for x in X.split('_')]\n",
        "    act = torch.nn.Hardswish\n",
        "    model = LeViT(\n",
        "        patch_size=16,\n",
        "        embed_dim=embed_dim,\n",
        "        num_heads=num_heads,\n",
        "        key_dim=[D] * 3,\n",
        "        depth=depth,\n",
        "        attn_ratio=[2, 2, 2],\n",
        "        mlp_ratio=[2, 2, 2],\n",
        "        down_ops=[\n",
        "            #('Subsample',key_dim, num_heads, attn_ratio, mlp_ratio, stride)\n",
        "            ['Subsample', D, embed_dim[0] // D, 4, 2, 2],\n",
        "            ['Subsample', D, embed_dim[1] // D, 4, 2, 2],\n",
        "        ],\n",
        "        attention_activation=act,\n",
        "        mlp_activation=act,\n",
        "        hybrid_backbone=b16(embed_dim[0], activation=act),\n",
        "        num_classes=num_classes,\n",
        "        drop_path=drop_path,\n",
        "        distillation=distillation\n",
        "    )\n",
        "    if pretrained:\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            weights, map_location='cpu')\n",
        "        model.load_state_dict(checkpoint['model'])\n",
        "    if fuse:\n",
        "        replace_batchnorm(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for name in specification:\n",
        "        net = globals()[name](fuse=True, pretrained=True)\n",
        "        net.eval()\n",
        "        net(torch.randn(4, 3, 224, 224))\n",
        "        print(name,\n",
        "              net.FLOPS, 'FLOPs',\n",
        "              sum(p.numel() for p in net.parameters() if p.requires_grad), 'parameters')\n"
      ],
      "metadata": {
        "id": "0-s22pU2pHcH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Model\n",
        "from timm.layers import activations, trunc_normal_\n",
        "import torch\n",
        "import itertools\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# from timm.models.vision_transformer import trunc_normal_\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import time\n",
        "import numpy as np\n",
        "\n",
        "__all__ = [\"LeViT\"]\n",
        "\n",
        "specification = {\n",
        "    'LeViT_384': {\n",
        "        'C': '384_512_768', 'D': 32, 'N': '6_9_12', 'X': '4_4_4', 'drop_path': 0.1,\n",
        "        'weights': 'https://dl.fbaipublicfiles.com/LeViT/LeViT-384-9bdaf2e2.pth'},\n",
        "}\n",
        "\n",
        "#__all__ = [specification.keys()]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "@register_model  ## No Mask\n",
        "def Build_LeViT_UNet_384(num_classes=4, distillation=True,\n",
        "              pretrained=False, fuse=False):\n",
        "    return model_factory_v3_NM(**specification['LeViT_384'], num_classes=num_classes,\n",
        "                         distillation=distillation, pretrained=pretrained, fuse=fuse)\n",
        "\n",
        "FLOPS_COUNTER = 0\n",
        "\n",
        "\n",
        "class Conv2d_BN(torch.nn.Sequential):\n",
        "    def __init__(self, a, b, ks=1, stride=1, pad=0, dilation=1,\n",
        "                 groups=1, bn_weight_init=1, resolution=-10000):\n",
        "        super().__init__()\n",
        "        self.add_module('c', torch.nn.Conv2d(\n",
        "            a, b, ks, stride, pad, dilation, groups, bias=False))\n",
        "        bn = torch.nn.BatchNorm2d(b)\n",
        "        torch.nn.init.constant_(bn.weight, bn_weight_init)\n",
        "        torch.nn.init.constant_(bn.bias, 0)\n",
        "        self.add_module('bn', bn)\n",
        "\n",
        "        global FLOPS_COUNTER\n",
        "        output_points = ((resolution + 2 * pad - dilation *\n",
        "                          (ks - 1) - 1) // stride + 1)**2\n",
        "        FLOPS_COUNTER += a * b * output_points * (ks**2) // groups\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fuse(self):\n",
        "        c, bn = self._modules.values()\n",
        "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
        "        w = c.weight * w[:, None, None, None]\n",
        "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
        "            (bn.running_var + bn.eps)**0.5\n",
        "        m = torch.nn.Conv2d(w.size(1), w.size(\n",
        "            0), w.shape[2:], stride=self.c.stride, padding=self.c.padding, dilation=self.c.dilation, groups=self.c.groups)\n",
        "        m.weight.data.copy_(w)\n",
        "        m.bias.data.copy_(b)\n",
        "        return m\n",
        "\n",
        "\n",
        "class Linear_BN(torch.nn.Sequential):\n",
        "    def __init__(self, a, b, bn_weight_init=1, resolution=-100000):\n",
        "        super().__init__()\n",
        "        self.add_module('c', torch.nn.Linear(a, b, bias=False))\n",
        "        bn = torch.nn.BatchNorm1d(b)\n",
        "        torch.nn.init.constant_(bn.weight, bn_weight_init)\n",
        "        torch.nn.init.constant_(bn.bias, 0)\n",
        "        self.add_module('bn', bn)\n",
        "\n",
        "        global FLOPS_COUNTER\n",
        "        output_points = resolution**2\n",
        "        FLOPS_COUNTER += a * b * output_points\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fuse(self):\n",
        "        l, bn = self._modules.values()\n",
        "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
        "        w = l.weight * w[:, None]\n",
        "        b = bn.bias - bn.running_mean * bn.weight / \\\n",
        "            (bn.running_var + bn.eps)**0.5\n",
        "        m = torch.nn.Linear(w.size(1), w.size(0))\n",
        "        m.weight.data.copy_(w)\n",
        "        m.bias.data.copy_(b)\n",
        "        return m\n",
        "\n",
        "    def forward(self, x):\n",
        "        l, bn = self._modules.values()\n",
        "        x = l(x)\n",
        "        return bn(x.flatten(0, 1)).reshape_as(x)\n",
        "\n",
        "\n",
        "class BN_Linear(torch.nn.Sequential):\n",
        "    def __init__(self, a, b, bias=True, std=0.02):\n",
        "        super().__init__()\n",
        "        self.add_module('bn', torch.nn.BatchNorm1d(a))\n",
        "        l = torch.nn.Linear(a, b, bias=bias)\n",
        "        trunc_normal_(l.weight, std=std)\n",
        "        if bias:\n",
        "            torch.nn.init.constant_(l.bias, 0)\n",
        "        self.add_module('l', l)\n",
        "        global FLOPS_COUNTER\n",
        "        FLOPS_COUNTER += a * b\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def fuse(self):\n",
        "        bn, l = self._modules.values()\n",
        "        w = bn.weight / (bn.running_var + bn.eps)**0.5\n",
        "        b = bn.bias - self.bn.running_mean * \\\n",
        "            self.bn.weight / (bn.running_var + bn.eps)**0.5\n",
        "        w = l.weight * w[None, :]\n",
        "        if l.bias is None:\n",
        "            b = b @ self.l.weight.T\n",
        "        else:\n",
        "            b = (l.weight @ b[:, None]).view(-1) + self.l.bias\n",
        "        m = torch.nn.Linear(w.size(1), w.size(0))\n",
        "        m.weight.data.copy_(w)\n",
        "        m.bias.data.copy_(b)\n",
        "        return m\n",
        "\n",
        "def b16(n, activation, resolution=224):\n",
        "    return torch.nn.Sequential(\n",
        "        Conv2d_BN(1, n // 8, 3, 2, 1, resolution=resolution), # aping num_channel\n",
        "        activation(),\n",
        "        Conv2d_BN(n // 8, n // 4, 3, 2, 1, resolution=resolution // 2),\n",
        "        activation(),\n",
        "        Conv2d_BN(n // 4, n // 2, 3, 2, 1, resolution=resolution // 4),\n",
        "        activation(),\n",
        "        Conv2d_BN(n // 2, n, 3, 2, 1, resolution=resolution // 8))\n",
        "\n",
        "\n",
        "class Residual(torch.nn.Module):\n",
        "    def __init__(self, m, drop):\n",
        "        super().__init__()\n",
        "        self.m = m\n",
        "        self.drop = drop\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training and self.drop > 0:\n",
        "            return x + self.m(x) * torch.rand(x.size(0), 1, 1,\n",
        "                                              device=x.device).ge_(self.drop).div(1 - self.drop).detach()\n",
        "        else:\n",
        "            return x + self.m(x)\n",
        "\n",
        "\n",
        "class Attention(torch.nn.Module):\n",
        "    def __init__(self, dim, key_dim, num_heads=8,\n",
        "                 attn_ratio=4,\n",
        "                 activation=None,\n",
        "                 resolution=14):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = key_dim ** -0.5\n",
        "        self.key_dim = key_dim\n",
        "        self.nh_kd = nh_kd = key_dim * num_heads\n",
        "        self.d = int(attn_ratio * key_dim)\n",
        "        self.dh = int(attn_ratio * key_dim) * num_heads\n",
        "        self.attn_ratio = attn_ratio\n",
        "        h = self.dh + nh_kd * 2\n",
        "        self.qkv = Linear_BN(dim, h, resolution=resolution)\n",
        "        self.proj = torch.nn.Sequential(activation(), Linear_BN(\n",
        "            self.dh, dim, bn_weight_init=0, resolution=resolution))\n",
        "\n",
        "        points = list(itertools.product(range(resolution), range(resolution)))\n",
        "        N = len(points)\n",
        "        attention_offsets = {}\n",
        "        idxs = []\n",
        "        for p1 in points:\n",
        "            for p2 in points:\n",
        "                offset = (abs(p1[0] - p2[0]), abs(p1[1] - p2[1]))\n",
        "                if offset not in attention_offsets:\n",
        "                    attention_offsets[offset] = len(attention_offsets)\n",
        "                idxs.append(attention_offsets[offset])\n",
        "        self.attention_biases = torch.nn.Parameter(\n",
        "            torch.zeros(num_heads, len(attention_offsets)))\n",
        "        self.register_buffer('attention_bias_idxs',\n",
        "                             torch.LongTensor(idxs).view(N, N))\n",
        "\n",
        "        global FLOPS_COUNTER\n",
        "        #queries * keys\n",
        "        FLOPS_COUNTER += num_heads * (resolution**4) * key_dim\n",
        "        # softmax\n",
        "        FLOPS_COUNTER += num_heads * (resolution**4)\n",
        "        #attention * v\n",
        "        FLOPS_COUNTER += num_heads * self.d * (resolution**4)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def train(self, mode=True):\n",
        "        super().train(mode)\n",
        "        if mode and hasattr(self, 'ab'):\n",
        "            del self.ab\n",
        "        else:\n",
        "            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n",
        "\n",
        "    def forward(self, x):  # x (B,N,C)\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x)\n",
        "        q, k, v = qkv.view(B, N, self.num_heads, -\n",
        "                           1).split([self.key_dim, self.key_dim, self.d], dim=3)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3)\n",
        "\n",
        "        attn = (\n",
        "            (q @ k.transpose(-2, -1)) * self.scale\n",
        "            +\n",
        "            (self.attention_biases[:, self.attention_bias_idxs]\n",
        "             if self.training else self.ab)\n",
        "        )\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, self.dh)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Subsample(torch.nn.Module):\n",
        "    def __init__(self, stride, resolution):\n",
        "        super().__init__()\n",
        "        self.stride = stride\n",
        "        self.resolution = resolution\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        x = x.view(B, self.resolution, self.resolution, C)[\n",
        "            :, ::self.stride, ::self.stride].reshape(B, -1, C)\n",
        "        return x\n",
        "\n",
        "\n",
        "class AttentionSubsample(torch.nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, key_dim, num_heads=8,\n",
        "                 attn_ratio=2,\n",
        "                 activation=None,\n",
        "                 stride=2,\n",
        "                 resolution=14, resolution_=7):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.scale = key_dim ** -0.5\n",
        "        self.key_dim = key_dim\n",
        "        self.nh_kd = nh_kd = key_dim * num_heads\n",
        "        self.d = int(attn_ratio * key_dim)\n",
        "        self.dh = int(attn_ratio * key_dim) * self.num_heads\n",
        "        self.attn_ratio = attn_ratio\n",
        "        self.resolution_ = resolution_\n",
        "        self.resolution_2 = resolution_**2\n",
        "        h = self.dh + nh_kd\n",
        "        self.kv = Linear_BN(in_dim, h, resolution=resolution)\n",
        "\n",
        "        self.q = torch.nn.Sequential(\n",
        "            Subsample(stride, resolution),\n",
        "            Linear_BN(in_dim, nh_kd, resolution=resolution_))\n",
        "        self.proj = torch.nn.Sequential(activation(), Linear_BN(\n",
        "            self.dh, out_dim, resolution=resolution_))\n",
        "\n",
        "        self.stride = stride\n",
        "        self.resolution = resolution\n",
        "        points = list(itertools.product(range(resolution), range(resolution)))\n",
        "        points_ = list(itertools.product(\n",
        "            range(resolution_), range(resolution_)))\n",
        "        N = len(points)\n",
        "        N_ = len(points_)\n",
        "        attention_offsets = {}\n",
        "        idxs = []\n",
        "        for p1 in points_:\n",
        "            for p2 in points:\n",
        "                size = 1\n",
        "                offset = (\n",
        "                    abs(p1[0] * stride - p2[0] + (size - 1) / 2),\n",
        "                    abs(p1[1] * stride - p2[1] + (size - 1) / 2))\n",
        "                if offset not in attention_offsets:\n",
        "                    attention_offsets[offset] = len(attention_offsets)\n",
        "                idxs.append(attention_offsets[offset])\n",
        "        self.attention_biases = torch.nn.Parameter(\n",
        "            torch.zeros(num_heads, len(attention_offsets)))\n",
        "        self.register_buffer('attention_bias_idxs',\n",
        "                             torch.LongTensor(idxs).view(N_, N))\n",
        "\n",
        "        global FLOPS_COUNTER\n",
        "        #queries * keys\n",
        "        FLOPS_COUNTER += num_heads * \\\n",
        "            (resolution**2) * (resolution_**2) * key_dim\n",
        "        # softmax\n",
        "        FLOPS_COUNTER += num_heads * (resolution**2) * (resolution_**2)\n",
        "        #attention * v\n",
        "        FLOPS_COUNTER += num_heads * \\\n",
        "            (resolution**2) * (resolution_**2) * self.d\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def train(self, mode=True):\n",
        "        super().train(mode)\n",
        "        if mode and hasattr(self, 'ab'):\n",
        "            del self.ab\n",
        "        else:\n",
        "            self.ab = self.attention_biases[:, self.attention_bias_idxs]\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        k, v = self.kv(x).view(B, N, self.num_heads, -\n",
        "                               1).split([self.key_dim, self.d], dim=3)\n",
        "        k = k.permute(0, 2, 1, 3)  # BHNC\n",
        "        v = v.permute(0, 2, 1, 3)  # BHNC\n",
        "        q = self.q(x).view(B, self.resolution_2, self.num_heads,\n",
        "                           self.key_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale + \\\n",
        "            (self.attention_biases[:, self.attention_bias_idxs]\n",
        "             if self.training else self.ab)\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, -1, self.dh)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "class Conv2dReLU(nn.Sequential):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            padding=0,\n",
        "            stride=1,\n",
        "            use_batchnorm=True,\n",
        "    ):\n",
        "        conv = nn.Conv2d(\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            bias=not (use_batchnorm),\n",
        "        )\n",
        "        relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        super(Conv2dReLU, self).__init__(conv, bn, relu)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            #skip_channels=0,\n",
        "            use_batchnorm=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.conv1 = Conv2dReLU(\n",
        "            #in_channels + skip_channels,\n",
        "            in_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        self.conv2 = Conv2dReLU(\n",
        "            out_channels,\n",
        "            out_channels,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            use_batchnorm=use_batchnorm,\n",
        "        )\n",
        "        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "\n",
        "    def forward(self, x, skip=None):\n",
        "        x = self.up(x)\n",
        "        if skip is not None:\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        return x\n",
        "\n",
        "class SegmentationHead(nn.Sequential):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, upsampling=1):\n",
        "        conv2d = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=kernel_size // 2)\n",
        "        upsampling = nn.UpsamplingBilinear2d(scale_factor=upsampling) if upsampling > 1 else nn.Identity()\n",
        "        super().__init__(conv2d, upsampling)\n",
        "\n",
        "\n",
        "\n",
        "##################################\n",
        "def model_factory_v3_NM(C, D, X, N, drop_path, weights,\n",
        "                  num_classes, distillation, pretrained, fuse):\n",
        "    embed_dim = [int(x) for x in C.split('_')]\n",
        "    num_heads = [int(x) for x in N.split('_')]\n",
        "    depth = [int(x) for x in X.split('_')]\n",
        "    act = torch.nn.Hardswish\n",
        "    model = LeViT_UNet_384(\n",
        "        patch_size=16,\n",
        "        embed_dim=embed_dim,\n",
        "        num_heads=num_heads,\n",
        "        key_dim=[D] * 3,\n",
        "        depth=depth,\n",
        "        attn_ratio=[2, 2, 2],\n",
        "        mlp_ratio=[2, 2, 2],\n",
        "        down_ops=[\n",
        "            #('Subsample',key_dim, num_heads, attn_ratio, mlp_ratio, stride)\n",
        "            ['Subsample', D, embed_dim[0] // D, 4, 2, 2],\n",
        "            ['Subsample', D, embed_dim[1] // D, 4, 2, 2],\n",
        "        ],\n",
        "        attention_activation=act,\n",
        "        mlp_activation=act,\n",
        "        hybrid_backbone=None,\n",
        "        num_classes=num_classes,  ### aping: set number class\n",
        "        drop_path=drop_path,\n",
        "        distillation=distillation\n",
        "    )\n",
        "    if pretrained:\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            weights, map_location='cpu')\n",
        "        ## aping: load according the model items\n",
        "        checkpoint_model = checkpoint['model']\n",
        "        # checkpoint_model['cnn_b1']=checkpoint_model.pop('patch_embed.0.c.weight')\n",
        "        all_pre_keys = checkpoint_model.keys()\n",
        "        re_str = ['patch_embed.2', 'patch_embed.2', 'patch_embed.4', 'patch_embed.6']\n",
        "        new_str = ['cnn_b2.0', 'cnn_b2.0', 'cnn_b3.0', 'cnn_b4.0']\n",
        "\n",
        "        for i, search_str in enumerate(re_str):\n",
        "            for item in list(all_pre_keys):\n",
        "                if search_str in item:\n",
        "                    replace_name = item.replace(re_str[i], new_str[i])\n",
        "                    # print(replace_name)\n",
        "                    checkpoint_model[replace_name]=checkpoint_model.pop(item)\n",
        "\n",
        "        re_trans = ['blocks']\n",
        "        for i, search_str in enumerate(re_trans):\n",
        "            for item in list(all_pre_keys):\n",
        "                if search_str in item:\n",
        "                    idx = int(item.split('.')[1])\n",
        "                    if (idx < 8):\n",
        "                        replace_name = item.replace('blocks', 'block_1')\n",
        "                        print('1: ', replace_name)\n",
        "                        checkpoint_model[replace_name]=checkpoint_model.pop(item)\n",
        "                    elif (8 <= idx < 18):\n",
        "                        replace_name = item.replace('blocks', 'block_2')\n",
        "                        print('2: ',replace_name)\n",
        "                        checkpoint_model[replace_name]=checkpoint_model.pop(item)\n",
        "                    else:\n",
        "                        replace_name = item.replace('blocks', 'block_3')\n",
        "                        print('3: ',replace_name)\n",
        "                        checkpoint_model[replace_name]=checkpoint_model.pop(item)\n",
        "\n",
        "\n",
        "        ## preposcess the pretrained model\n",
        "        model_dict = model.state_dict()\n",
        "        pretrained_dict = {k:v for k, v in checkpoint_model.items() if k in model_dict}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        model.load_state_dict(model_dict) ## model.blocks[0].m.qkv.c   == model.block_1[0].m.qkv.c\n",
        "\n",
        "\n",
        "\n",
        "    if fuse:\n",
        "        replace_batchnorm(model)\n",
        "\n",
        "    return model\n",
        "\n",
        "class LeViT_UNet_384(torch.nn.Module):\n",
        "    \"\"\" Vision Transformer with support for patch or hybrid CNN input stage\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, img_size=224, ## aping: should change\n",
        "                 patch_size=16,\n",
        "                 in_chans=3,\n",
        "                 num_classes=9,\n",
        "                 embed_dim=[192],\n",
        "                 key_dim=[64],\n",
        "                 depth=[12],\n",
        "                 num_heads=[3],\n",
        "                 attn_ratio=[2],\n",
        "                 mlp_ratio=[2],\n",
        "                 hybrid_backbone=None,\n",
        "                 down_ops=[],\n",
        "                 attention_activation=torch.nn.Hardswish,\n",
        "                 mlp_activation=torch.nn.Hardswish,\n",
        "                 distillation=True,\n",
        "                 drop_path=0):\n",
        "        super().__init__()\n",
        "        global FLOPS_COUNTER\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = embed_dim[-1]\n",
        "        self.embed_dim = embed_dim\n",
        "        self.distillation = distillation\n",
        "\n",
        "        # self.patch_embed = hybrid_backbone ## aping: CNN  # aping num_channel\n",
        "        n = 384 ## cnn num channel\n",
        "        activation = torch.nn.Hardswish\n",
        "        self.cnn_b1 = torch.nn.Sequential(\n",
        "            Conv2d_BN(1, n // 8, 3, 2, 1, resolution=img_size), activation())\n",
        "        self.cnn_b2 = torch.nn.Sequential(\n",
        "            Conv2d_BN(n // 8, n // 4, 3, 2, 1, resolution=img_size // 2), activation())\n",
        "        self.cnn_b3 = torch.nn.Sequential(\n",
        "            Conv2d_BN(n // 4, n // 2, 3, 2, 1, resolution=img_size // 4), activation())\n",
        "        self.cnn_b4 = torch.nn.Sequential(\n",
        "            Conv2d_BN(n // 2, n, 3, 2, 1, resolution=img_size // 8))\n",
        "\n",
        "\n",
        "        self.decoderBlock_1 = DecoderBlock(2048, 512) #->att+cnn:384+512+768+384=2048==1280+256\n",
        "        self.decoderBlock_2 = DecoderBlock(704, 256) #->28->56, 512+9+192=713\n",
        "        self.decoderBlock_3 = DecoderBlock(352, 128) #->56->112, 256+9+96=361\n",
        "        self.segmentation_head = SegmentationHead(176, self.num_classes, kernel_size=3, upsampling=2)\n",
        "\n",
        "\n",
        "        self.blocks = [] ## attention block\n",
        "        down_ops.append([''])\n",
        "        resolution = img_size // patch_size\n",
        "        for i, (ed, kd, dpth, nh, ar, mr, do) in enumerate(\n",
        "                zip(embed_dim, key_dim, depth, num_heads, attn_ratio, mlp_ratio, down_ops)):\n",
        "            print(i)\n",
        "            for _ in range(dpth):\n",
        "                self.blocks.append(\n",
        "                    Residual(Attention(\n",
        "                        ed, kd, nh,\n",
        "                        attn_ratio=ar,\n",
        "                        activation=attention_activation,\n",
        "                        resolution=resolution,\n",
        "                    ), drop_path))\n",
        "                if mr > 0:\n",
        "                    h = int(ed * mr)\n",
        "                    self.blocks.append(\n",
        "                        Residual(torch.nn.Sequential(\n",
        "                            Linear_BN(ed, h, resolution=resolution),\n",
        "                            mlp_activation(),\n",
        "                            Linear_BN(h, ed, bn_weight_init=0,\n",
        "                                      resolution=resolution),\n",
        "                        ), drop_path))\n",
        "            if do[0] == 'Subsample':\n",
        "                #('Subsample',key_dim, num_heads, attn_ratio, mlp_ratio, stride)\n",
        "                resolution_ = (resolution - 1) // do[5] + 1\n",
        "                self.blocks.append(\n",
        "                    AttentionSubsample(\n",
        "                        *embed_dim[i:i + 2], key_dim=do[1], num_heads=do[2],\n",
        "                        attn_ratio=do[3],\n",
        "                        activation=attention_activation,\n",
        "                        stride=do[5],\n",
        "                        resolution=resolution,\n",
        "                        resolution_=resolution_))\n",
        "                resolution = resolution_\n",
        "                if do[4] > 0:  # mlp_ratio\n",
        "                    h = int(embed_dim[i + 1] * do[4])\n",
        "                    self.blocks.append(\n",
        "                        Residual(torch.nn.Sequential(\n",
        "                            Linear_BN(embed_dim[i + 1], h,\n",
        "                                      resolution=resolution),\n",
        "                            mlp_activation(),\n",
        "                            Linear_BN(\n",
        "                                h, embed_dim[i + 1], bn_weight_init=0, resolution=resolution),\n",
        "                        ), drop_path))\n",
        "        self.blocks = torch.nn.Sequential(*self.blocks)\n",
        "        ## divid the blocks\n",
        "        self.block_1 = self.blocks[0:8]\n",
        "        self.block_2 = self.blocks[8:18]\n",
        "        self.block_3 = self.blocks[18:28]\n",
        "\n",
        "        del self.blocks\n",
        "\n",
        "        ## aping: upsampling\n",
        "        self.up = nn.UpsamplingBilinear2d(scale_factor=2)\n",
        "\n",
        "\n",
        "        self.FLOPS = FLOPS_COUNTER\n",
        "        FLOPS_COUNTER = 0\n",
        "\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        return {x for x in self.state_dict().keys() if 'attention_biases' in x}\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x_cnn_1 = self.cnn_b1(x) # torch.Size([4, 1, 512, 512])->([4, 32, 256, 256])\n",
        "\n",
        "        x_cnn_2 = self.cnn_b2(x_cnn_1) #-> ([4, 64, 128, 128])\n",
        "\n",
        "        x_cnn_3 = self.cnn_b3(x_cnn_2) #->([4, 128, 64, 64])\n",
        "\n",
        "        x_cnn = self.cnn_b4(x_cnn_3) # ([4, 256, 32, 32])\n",
        "\n",
        "\n",
        "\n",
        "        x = x_cnn.flatten(2).transpose(1, 2) # torch.Size([4, 196, 256])\n",
        "\n",
        "\n",
        "        ## aping\n",
        "        x = self.block_1(x) # torch.Size([4, 196, 384])-->Nx256x14x14\n",
        "        x_num, x_len = x.shape[0], x.shape[1]\n",
        "\n",
        "        x_r_1 = x.reshape(x_num, int(x_len**0.5), int(x_len**0.5), -1)\n",
        "        x_r_1 = x_r_1.permute(0,3,1,2)\n",
        "\n",
        "        x = self.block_2(x) # downsample + att  torch.Size([4, 49, 384])\n",
        "        x_num, x_len = x.shape[0], x.shape[1]\n",
        "\n",
        "        x_r_2 = x.reshape(x_num, int(x_len**0.5), int(x_len**0.5), -1)\n",
        "        x_r_2 = x_r_2.permute(0,3,1,2)\n",
        "        ## upsampling\n",
        "        x_r_2_up = self.up(x_r_2)\n",
        "\n",
        "\n",
        "        x = self.block_3(x) # torch.Size([4, 16, 512])\n",
        "        x_num, x_len = x.shape[0], x.shape[1]\n",
        "\n",
        "        x_r_3 = x.reshape(x_num, int(x_len**0.5), int(x_len**0.5), -1)\n",
        "        x_r_3 = x_r_3.permute(0,3,1,2)\n",
        "        ## upsampling\n",
        "        x_r_3_up = self.up(x_r_3)\n",
        "        x_r_3_up = self.up(x_r_3_up)\n",
        "\n",
        "        ## aping: resize the feature maps\n",
        "        if (x_r_2_up.shape  != x_r_3_up.shape):\n",
        "            x_r_3_up = F.interpolate(x_r_3_up, size=x_r_2_up.shape[2:], mode=\"bilinear\",  align_corners=True)\n",
        "        att_all = torch.cat([ x_r_1, x_r_2_up, x_r_3_up], dim=1) # 384+512+768\n",
        "\n",
        "        x_att_all = torch.cat([x_cnn, att_all], dim=1) ## torch.Size([4, 1408, 32, 32])\n",
        "        decoder_feature = self.decoderBlock_1(x_att_all) # x_att_all: ([4, 1408, 32, 32])->512\n",
        "\n",
        "        decoder_feature = torch.cat([decoder_feature, x_cnn_3], dim=1) #:(640+9)x64x64\n",
        "        decoder_feature = self.decoderBlock_2(decoder_feature)\n",
        "\n",
        "        decoder_feature = torch.cat([decoder_feature, x_cnn_2], dim=1) # ([4, (320+9), 128, 128])\n",
        "        decoder_feature = self.decoderBlock_3(decoder_feature)\n",
        "\n",
        "        decoder_feature = torch.cat([decoder_feature, x_cnn_1], dim=1) # ([4, 169, 256, 256])\n",
        "\n",
        "        logits = self.segmentation_head(decoder_feature) ## torch.Size([4, 2, 224, 224])\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "#################################\n",
        "def netParams(model):\n",
        "    \"\"\"\n",
        "    computing total network parameters\n",
        "    args:\n",
        "       model: model\n",
        "    return: the number of parameters\n",
        "    \"\"\"\n",
        "    total_paramters = 0\n",
        "    for parameter in model.parameters():\n",
        "        i = len(parameter.size())\n",
        "        p = 1\n",
        "        for j in range(i):\n",
        "            p *= parameter.size(j)\n",
        "        total_paramters += p\n",
        "\n",
        "    return total_paramters\n",
        "\n",
        "\n",
        "from ptflops import get_model_complexity_info\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    for name in specification:\n",
        "        net = globals()[name](fuse=True, pretrained=False)\n",
        "        #net = Build_LeViT_UNet_384(num_classes=9, pretrained=True, fuse=True)\n",
        "        net.eval()\n",
        "        output = net(torch.randn(1, 1, 224, 224))\n",
        "\n",
        "        net(torch.randn(1, 1, 224, 224)) #NCHW\n",
        "        ## cal para FLOPS\n",
        "        macs, params = get_model_complexity_info(net, (1, 224, 224), as_strings=True,\n",
        "                                                print_per_layer_stat=False, verbose=False)\n",
        "        print('{:<30}  {:<8}'.format('Computational complexity: ', macs ))\n",
        "        print('{:<30}  {:<8}'.format('Number of parameters: ', params ))\n",
        "        # print(net)\n",
        "        print(name,\n",
        "              net.FLOPS, 'FLOPs',\n",
        "              sum(p.numel() for p in net.parameters() if p.requires_grad), 'parameters')\n",
        "        total_paramters = netParams(net)\n",
        "        print(\"the number of parameters: %d ==> %.2f M\" % (total_paramters, (total_paramters / 1e6)))\n",
        "        print(net)\n"
      ],
      "metadata": {
        "id": "wEFQjiL5q3KY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title main2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert the images and masks to tensors\n",
        "transform = ToTensor()\n",
        "images = [transform(image) for image in train_images]\n",
        "masks = [transform(mask) for mask in train_masks]\n",
        "\n",
        "# Resize masks to remove the extra dimension if present\n",
        "masks = [mask.squeeze(dim=0) for mask in masks]\n",
        "\n",
        "# Split the data into train and validation sets\n",
        "train_images, val_images, train_masks, val_masks = train_test_split(images, masks, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert the train and validation image and mask lists to tensors and create datasets\n",
        "train_dataset = TensorDataset(torch.stack(train_images), torch.stack(train_masks))\n",
        "val_dataset = TensorDataset(torch.stack(val_images), torch.stack(val_masks))\n",
        "\n",
        "# Create DataLoaders for batch processing\n",
        "batch_size = 16\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Assuming you have defined and initialized your model\n",
        "model = Build_LeViT_UNet_384(num_classes=2, distillation=True, pretrained=False, fuse=False)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 150\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)  # Move model to the device\n",
        "\n",
        "def train_step(batch_images, batch_masks):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(batch_images)\n",
        "    outputs = nn.functional.interpolate(outputs, size=batch_masks.shape[1:], mode='bilinear', align_corners=False)\n",
        "    loss = criterion(outputs, batch_masks.long())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return loss.item()\n",
        "\n",
        "def val_step(batch_images, batch_masks):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(batch_images)\n",
        "        outputs = nn.functional.interpolate(outputs, size=batch_masks.shape[1:], mode='bilinear', align_corners=False)\n",
        "        loss = criterion(outputs, batch_masks.long())\n",
        "    return loss.item()\n",
        "\n",
        "# Use the fit function for training and validation\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Initialize lists to store training and validation accuracies\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "# Initialize variables for tracking correct predictions and total examples\n",
        "train_correct = 0\n",
        "train_total = 0\n",
        "val_correct = 0\n",
        "val_total = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_loss = 0.0\n",
        "    val_loss = 0.0\n",
        "    train_correct = 0  # Reset the correct predictions for each epoch\n",
        "    train_total = 0    # Reset the total examples for each epoch\n",
        "    val_correct = 0    # Reset the correct predictions for each epoch\n",
        "    val_total = 0      # Reset the total examples for each epoch\n",
        "\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    for batch_images, batch_masks in tqdm(train_dataloader, desc=f\"Epoch {epoch+1} - Training\"):\n",
        "        batch_images = batch_images.to(device)\n",
        "        batch_masks = batch_masks.to(device)\n",
        "        loss = train_step(batch_images, batch_masks)\n",
        "        train_loss += loss\n",
        "\n",
        "        outputs = model(batch_images)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        train_total += batch_masks.numel()\n",
        "        train_correct += (predicted == batch_masks).sum().item()\n",
        "\n",
        "\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_images, batch_masks in tqdm(val_dataloader, desc=f\"Epoch {epoch+1} - Validation\"):\n",
        "            batch_images = batch_images.to(device)\n",
        "            batch_masks = batch_masks.to(device)\n",
        "            loss = val_step(batch_images, batch_masks)\n",
        "            val_loss += loss\n",
        "            outputs = model(batch_images)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            val_total += batch_masks.numel()\n",
        "            val_correct += (predicted == batch_masks).sum().item()\n",
        "\n",
        "    train_accuracy = train_correct / train_total\n",
        "    val_accuracy = val_correct / val_total\n",
        "\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    val_accuracies.append(val_accuracy)\n",
        "\n",
        "    # Print the average loss for the epoch\n",
        "    avg_train_loss = train_loss / len(train_dataloader)\n",
        "    avg_val_loss = val_loss / len(val_dataloader)\n",
        "    print(f\"Epoch {epoch+1} - Training Loss: {avg_train_loss:.4f} - Validation Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    # Display original images, masks, and predicted masks side by side\n",
        "    fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(10, 10))\n",
        "    axes[0, 0].set_title(\"Original Image\")\n",
        "    axes[0, 1].set_title(\"Original Mask\")\n",
        "    axes[0, 2].set_title(\"Predicted Mask\")\n",
        "\n",
        "    for i in range(3):\n",
        "        original_image = train_images[i].permute(1, 2, 0).cpu().numpy()\n",
        "        original_mask = train_masks[i].cpu().numpy()\n",
        "        predicted_mask = model(train_images[i].unsqueeze(0).to(device)).squeeze(0).argmax(dim=0).cpu().numpy()\n",
        "\n",
        "        axes[i, 0].imshow(original_image)\n",
        "        axes[i, 1].imshow(original_mask)\n",
        "        axes[i, 2].imshow(predicted_mask)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Plot the training and validation accuracies\n",
        "plt.plot(range(1, epochs+1), train_accuracies, label='Training Accuracy')\n",
        "plt.plot(range(1, epochs+1), val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Model Accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# After training, you can save the model if desired\n",
        "#torch.save(model.state_dict(), \"#\")\n",
        "\n"
      ],
      "metadata": {
        "id": "ymTFrg9rr1YD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}